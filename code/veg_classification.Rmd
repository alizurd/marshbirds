---
title: "Saltmarsh Habitat Classification Models"
author: "Alyssa Bueno"
date: "2025-08-02"
output: 
  pdf_document:
      highlight: tango
      keep_tex: true
      includes:
          in_header: preamble.tex
---

## Saltmarsh Habitat Classification
This code outlines 4 different model classification iterations, each differing by the input layers. 

1. NDWI + NDVI + PCA + NAIP + Brightness
2. NDWI + NDVI + PCA
3. NAIP + NDVI
4. NAIP only
5. PCA only

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/Users/alyssabueno/Desktop/marshbirdsoutput/round_9")

library(terra)
library(randomForest)
library(dplyr)
library(caret)
library(sf)
library(tidyr)
library(ggplot2)
library(tinytex)
library(xfun)

```

# Setup: Ingest the training data and set up the layers

```{r}

# load raster and training polygons

# this has the raster data    
raster_files <- c(
  "/Users/alyssabueno/Desktop/marshbirds_data/clips/clip_bx.tif",
  "/Users/alyssabueno/Desktop/marshbirds_data/clips/clip_li.tif",
  "/Users/alyssabueno/Desktop/marshbirds_data/clips/clip_gb.tif"
)

# this has the classes
shp_files <- c(
  "/Users/alyssabueno/Desktop/marshbirds_data/training_polygons/training_polygons_bx.shp",
  "/Users/alyssabueno/Desktop/marshbirds_data/training_polygons/training_polygons_li.shp",
  "/Users/alyssabueno/Desktop/marshbirds_data/training_polygons/training_polygons_gb.shp"
) 

# load them up
rasters <- lapply(raster_files, rast)
polygons <- lapply(shp_files, vect)


# -------------------------------
#   raster extraction and calcs 
# -------------------------------

all_extracted <- list()

for (i in 1:length(rasters)) {
  naip <- rasters[[i]]
  polygon <- polygons[[i]]

names(naip) <- paste0("naip", 1:4) # change name of naip bands layer

# calculate NDVI
ndvi <- (naip[[4]] - naip[[1]]) / (naip[[4]] + naip[[1]])
names(ndvi) <- "ndvi"

# calculate brightness
brightness <- (naip[[1]] + naip[[2]] + naip[[3]] + naip[[4]]) / 4
names(brightness) <- "brightness"

# calculate ndwi
ndwi <- (naip[[4]] - naip[[2]]) / (naip[[4]] + naip[[2]])
names(ndwi) <- "ndwi"

# now create the PCA 

all_for_pca <- c(naip, ndvi)  # add ndvi to the raster
vals <- values(all_for_pca)
vals <- vals[complete.cases(vals), ]
pca_pixels <- prcomp(vals, center = TRUE, scale. = TRUE)
pca <- predict(all_for_pca, pca_pixels, index = 1:5)  # for 5 PCs
names(pca) <- paste0("PCA", 1:5)


# 1. First Iteration: NDWI + NDVI + PCA + NAIP + Brightness

# stack em up
r_stack <- c(ndwi, ndvi, pca, naip, brightness)

# extract raster values for training polygons
extracted_i <- terra::extract(r_stack, polygon, df = TRUE)

# extract raster values for training polygons
extracted_i <- terra::extract(r_stack, polygon, df = TRUE)

# Add class label using polygon index
extracted_i$class <- polygon$class[extracted_i$ID]

# remove ID because it should NOT go into model training
extracted_i$ID <- NULL

all_extracted[[i]] <- extracted_i

cat("Processed site", i, "\n")
  
}

# Combine all sites
extracted <- bind_rows(all_extracted)

# clean data by removing rows with na values
extracted_clean <- na.omit(extracted)  # remove NAs

# sample 2000 per class
training_data <- extracted_clean %>%
  group_by(class) %>%
  sample_n(min(2000, n())) %>%  # Use min() to handle small classes
  ungroup()

# turn class column into factor
training_data$class <- factor(training_data$class)

# check the sampling balance in the classes
print(table(training_data$class))

```

```{r}

# ingest the test data
test_raster_files <- c(
  "/Users/alyssabueno/Desktop/marshbirds_data/test_clips/test_clips_bx.tif",
  "/Users/alyssabueno/Desktop/marshbirds_data/test_clips/test_clips_li.tif",
  "/Users/alyssabueno/Desktop/marshbirds_data/test_clips/test_clips_gb.tif"
)

test_shp_files <- c(
  "/Users/alyssabueno/Desktop/marshbirds_data/test_polygons/test_polygons_bx.shp",
  "/Users/alyssabueno/Desktop/marshbirds_data/test_polygons/test_polygons_li.shp",
  "/Users/alyssabueno/Desktop/marshbirds_data/test_polygons/test_polygons_gb.shp"
)

test_rasters  <- lapply(test_raster_files, rast)
test_polygons <- lapply(test_shp_files, vect)

# calculate PCA based on the same scale as the training data
# pca_pixels <- prcomp(vals, center = TRUE, scale. = TRUE)

# store r_stack for visualization later
r_stacks <- list()

test_extracted <- list()

for (i in 1:length(test_rasters)) {
  naip <- test_rasters[[i]]
  polygon <- test_polygons[[i]]

  names(naip) <- paste0("naip", 1:4)

  # --- match training calculations ---

  ndvi <- (naip[[4]] - naip[[1]]) / (naip[[4]] + naip[[1]])
  names(ndvi) <- "ndvi"

  brightness <- (naip[[1]] + naip[[2]] + naip[[3]] + naip[[4]]) / 4
  names(brightness) <- "brightness"

  ndwi <- (naip[[4]] - naip[[2]]) / (naip[[4]] + naip[[2]])
  names(ndwi) <- "ndwi"

  # --- apply SAME PCA model as training ---
  pca_input <- c(naip, ndvi)
  pca <- predict(pca_input, pca_pixels, index = 1:5)
  names(pca) <- paste0("PCA", 1:5)

  # full predictor stack
  r_stack <- c(ndwi, ndvi, pca, naip, brightness)
  
  # store r_stack for later visualization
  r_stacks[[i]] <- r_stack

  # --- extract ---
  extracted_i <- terra::extract(r_stack, polygon, df = TRUE)

  # add class label
  extracted_i$class <- polygon$class[extracted_i$ID]

  # drop ID
  extracted_i$ID <- NULL

  test_extracted[[i]] <- extracted_i

  cat("Processed TEST site", i, "\n")
}

# Combine all sites
extracted_test <- bind_rows(test_extracted)

# clean data by removing rows with na values
extracted_test_clean <- na.omit(extracted_test)  # remove NAs

# sample 2000 per class
test_data <- extracted_test_clean 

# %>%
#   group_by(class) %>%
#   sample_n(min(2000, n())) %>%  # Use min() to handle small classes
#   ungroup()

# turn class column into factor
test_data$class <- factor(test_data$class)

# check the sampling balance in the classes
print(table(test_data$class))

# train random forest model on training data
rf_model <- randomForest(class ~ ., 
                         data = training_data, 
                         ntree = 500)

# validation metrics
preds <- predict(rf_model, newdata = test_data)
accuracy <- mean(preds == test_data$class)
cat("Accuracy:", round(accuracy, 4), "\n")

print(rf_model)

# confusion matrix
confusionMatrix(preds, test_data$class)
```

```{r}
# visualizing test plots

pred_bx <- predict(r_stacks[[1]], rf_model)
plot(pred_bx)

```

# classifying the actual plots

```{r}
# load the new, unlabeled raster
setwd("~/Desktop/marshbirds_data")
files <- c(
  "~/Desktop/marshbirds_data/pred_clips/bellport_raster_200.tif",
  "~/Desktop/marshbirds_data/pred_clips/bx_li_raster_200.tif",
  "~/Desktop/marshbirds_data/pred_clips/dune_raster_200.tif",
  "~/Desktop/marshbirds_data/pred_clips/e_montauk_raster_200.tif",
  "~/Desktop/marshbirds_data/pred_clips/east_point_montauk_raster_200.tif",
  "~/Desktop/marshbirds_data/pred_clips/east_queens_raster_200.tif",
  "~/Desktop/marshbirds_data/pred_clips/eastern_end_montauk_raster_200.tif",
  "~/Desktop/marshbirds_data/pred_clips/gardiner_raster_200.tif",
  "~/Desktop/marshbirds_data/pred_clips/gilgo_raster_200.tif",
  "~/Desktop/marshbirds_data/pred_clips/hecksher_raster_200.tif",
  "~/Desktop/marshbirds_data/pred_clips/jones_raster_200.tif",
  "~/Desktop/marshbirds_data/pred_clips/ne_li_raster_200.tif",
  "~/Desktop/marshbirds_data/pred_clips/north_central_li_raster_200.tif",
  "~/Desktop/marshbirds_data/pred_clips/nw_li_raster_200.tif",
  "~/Desktop/marshbirds_data/pred_clips/nw_montauk_raster_200.tif",
  "~/Desktop/marshbirds_data/pred_clips/queens_raster_200.tif",
  "~/Desktop/marshbirds_data/pred_clips/s_central_li_raster_200.tif",
  "~/Desktop/marshbirds_data/pred_clips/s_montauk_raster_200.tif",
  "~/Desktop/marshbirds_data/pred_clips/sb_central_li_raster_200.tif",
  "~/Desktop/marshbirds_data/pred_clips/si_raster_200.tif",
  "~/Desktop/marshbirds_data/pred_clips/w_montauk_raster_200.tif"
)


# Custom output directory
out_dir <- "~/Desktop/marshbirdsoutput/round_11/"
dir.create(out_dir, showWarnings = FALSE)


# -----------------------------
# PROCESS EACH RASTER INDIVIDUALLY
# -----------------------------

for (f in files) {

  message("Processing: ", f)

  r <- rast(f)

  # --- Derived predictors ---
  ndvi <- (r[[4]] - r[[1]]) / (r[[4]] + r[[1]])
  names(ndvi) <- "ndvi"

  ndwi <- (r[[4]] - r[[2]]) / (r[[4]] + r[[2]])
  names(ndwi) <- "ndwi"

  brightness <- (r[[1]] + r[[2]] + r[[3]] + r[[4]]) / 4
  names(brightness) <- "brightness"

  # PCA input (must match training)
  all_for_pca_pred <- c(r, ndvi)
  names(all_for_pca_pred) <- names(all_for_pca)

  pca_pred <- predict(all_for_pca_pred, pca_pixels, index = 1:5)
  names(pca_pred) <- paste0("PCA", 1:5)

  # Final predictor stack
  prediction_stack <- c(r, ndvi, ndwi, brightness, pca_pred)

  # Make sure the names match training stack exactly
  names(prediction_stack) <- names(r_stack)

  # Predict classes
  classified <- predict(prediction_stack, rf_model, na.rm = TRUE)

  # Output file name
  base <- tools::file_path_sans_ext(basename(f))
  out_file <- file.path(out_dir, paste0(base, "_classified.tif"))

  writeRaster(classified, out_file, overwrite = TRUE)

  message("Saved: ", out_file)
}

message("ALL DONE âœ”")
```
``` {r}

print(rf_model)

```

# feature class importance and visualizations

```{r}
# ggplot(training_data, aes(x = ndwi, y = ndvi, color = class)) +
#   geom_point(alpha = 0.3) +
#   theme_minimal()
# 
# ggplot(training_data, aes(x = ndwi, fill = class)) +
#   geom_density(alpha = 0.4, color = NA) +
#   theme_minimal()
# 
# imp <- as.data.frame(rf_model$importance)
# imp$feature <- rownames(imp)
# 
# ggplot(imp, aes(x = reorder(feature, MeanDecreaseGini), y = MeanDecreaseGini)) +
#   geom_col(fill = "steelblue") +
#   coord_flip() +
#   labs(title = "Random Forest Variable Importance",
#        x = "Feature", y = "Mean Decrease Gini") +
#   theme_minimal()
```

# 2. Second Iteration: NAIP + NDVI + NDWI

```{r}
# stack em up
r_stack <- c(naip, ndvi, ndwi)

# extract raster values for training polygons
extracted <- terra::extract(r_stack, training_polygons, df = TRUE)

# turn class into a factor
training_polygons$class <- as.factor(training_polygons$class)

# Convert training_polygons to dataframe to get the class labels
poly_df <- as.data.frame(training_polygons)
poly_df$ID <- 1:nrow(poly_df)  # Add ID column to match extract output

# Join the class labels
extracted <- extracted %>%
  left_join(poly_df[, c("ID", "class")], by = "ID")

# clean data by removing rows with na values and remove ID column
extracted_clean <- na.omit(extracted[, -1])  # remove ID and NAs

# sample 2000 per class
training_data <- extracted_clean %>%
  group_by(class) %>%
  sample_n(min(2000, n())) %>%  # Use min() to handle small classes
  ungroup()

# turn class column into factor
training_data$class <- factor(training_data$class)

```

```{r}
# split training and test data
set.seed(342)
idx <- sample(seq_len(nrow(training_data)), size = 0.8 * nrow(training_data))
train_set <- training_data[idx, ]
test_set  <- training_data[-idx, ]

# train random forest model
rf_model <- randomForest(class ~ ., 
                         data = train_set, 
                         ntree = 500)

# validation metrics
preds <- predict(rf_model, newdata = test_set)
accuracy <- mean(preds == test_set$class)
cat("Accuracy:", round(accuracy, 4), "\n")

print(rf_model)

# confusion matrix
confusionMatrix(preds, test_set$class)
```

# classifying the plots

```{r}
# load the new, unlabeled raster
prediction_raster <- rast("~/Desktop/marshbirdsoutput/round_6/prediction_raster_round_6.tif")

# calculate NDVI for prediction raster
ndvi_pred <- (prediction_raster[[4]] - prediction_raster[[1]]) / 
  (prediction_raster[[4]] + prediction_raster[[1]])
names(ndvi_pred) <- "ndvi"

# calculation NDWI
ndwi_pred <- (prediction_raster[[4]] - prediction_raster[[2]]) / 
  (prediction_raster[[4]] + prediction_raster[[2]])
names(ndwi_pred) <- "ndwi"

# Calculate brightness
brightness_pred <- (prediction_raster[[1]] + prediction_raster[[2]] + 
                      prediction_raster[[3]] + prediction_raster[[4]]) / 4
names(brightness_pred) <- "brightness"

# Apply the pca
all_for_pca_pred <- c(prediction_raster, ndvi_pred)
names(all_for_pca_pred) <- names(all_for_pca)  # ensure exact match
pca_pred <- predict(all_for_pca_pred, pca_pixels, index = 1:5)
names(pca_pred) <- paste0("PCA", 1:5)

# stack it
prediction_stack <- c(prediction_raster, ndvi_pred, ndwi_pred)

# rename to match training names
names(prediction_stack) <- names(r_stack)

# apply the model to predict classes
classified <- predict(prediction_stack, rf_model, na.rm = TRUE)

# Define custom colors
colors <- c(
  "#a6d96a",  # hm  
  "#1a9641",  # lm  
  "#8c510a",  # md  
  "#3288bd",  # ow  
  "#fdae61",  # ph  
  "#969696",  # rd  
  "#762a83"   # up  
)



# Plot with colors
plot(classified, col = colors)

# save the output tif
# writeRaster(classified, "~/Desktop/marshbirdsoutput/round_6/classified_output.tif", overwrite = TRUE)

model <- rf_model$importance
print(model)
```

# feature class importance and visualizations

```{r}
ggplot(training_data, aes(x = naip4, y = ndvi, color = class)) +
  geom_point(alpha = 0.3) +
  theme_minimal()

ggplot(training_data, aes(x = naip4, fill = class)) +
  geom_density(alpha = 0.4, color = NA) +
  theme_minimal()

imp <- as.data.frame(rf_model$importance)
imp$feature <- rownames(imp)

ggplot(imp, aes(x = reorder(feature, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Random Forest Variable Importance",
       x = "Feature", y = "Mean Decrease Gini") +
  theme_minimal()
```

# 3. Third Iteration: NAIP + NDVI

```{r}
# stack em up
r_stack <- c(naip, ndvi)

# extract raster values for training polygons
extracted <- terra::extract(r_stack, training_polygons, df = TRUE)

# turn class into a factor
training_polygons$class <- as.factor(training_polygons$class)

# Convert training_polygons to dataframe to get the class labels
poly_df <- as.data.frame(training_polygons)
poly_df$ID <- 1:nrow(poly_df)  # Add ID column to match extract output

# Join the class labels
extracted <- extracted %>%
  left_join(poly_df[, c("ID", "class")], by = "ID")

# clean data by removing rows with na values and remove ID column
extracted_clean <- na.omit(extracted[, -1])  # remove ID and NAs

# sample 2000 per class
training_data <- extracted_clean %>%
  group_by(class) %>%
  sample_n(min(2000, n())) %>%  # Use min() to handle small classes
  ungroup()

# turn class column into factor
training_data$class <- factor(training_data$class)

```

```{r}
# split training and test data
set.seed(342)
idx <- sample(seq_len(nrow(training_data)), size = 0.8 * nrow(training_data))
train_set <- training_data[idx, ]
test_set  <- training_data[-idx, ]

# train random forest model
rf_model <- randomForest(class ~ ., 
                         data = train_set, 
                         ntree = 500)

# validation metrics
preds <- predict(rf_model, newdata = test_set)
accuracy <- mean(preds == test_set$class)
cat("Accuracy:", round(accuracy, 4), "\n")

print(rf_model)

# confusion matrix
confusionMatrix(preds, test_set$class)
```

# classifying the plots

```{r}
# load the new, unlabeled raster
prediction_raster <- rast("~/Desktop/marshbirdsoutput/round_6/prediction_raster_round_6.tif")

# calculate NDVI for prediction raster
ndvi_pred <- (prediction_raster[[4]] - prediction_raster[[1]]) / 
  (prediction_raster[[4]] + prediction_raster[[1]])
names(ndvi_pred) <- "ndvi"

# calculation NDWI
ndwi_pred <- (prediction_raster[[4]] - prediction_raster[[2]]) / 
  (prediction_raster[[4]] + prediction_raster[[2]])
names(ndwi_pred) <- "ndwi"

# Calculate brightness
brightness_pred <- (prediction_raster[[1]] + prediction_raster[[2]] + 
                      prediction_raster[[3]] + prediction_raster[[4]]) / 4
names(brightness_pred) <- "brightness"

# Apply the pca
all_for_pca_pred <- c(prediction_raster, ndvi_pred)
names(all_for_pca_pred) <- names(all_for_pca)  # ensure exact match
pca_pred <- predict(all_for_pca_pred, pca_pixels, index = 1:5)
names(pca_pred) <- paste0("PCA", 1:5)

# stack it
prediction_stack <- c(prediction_raster, ndvi_pred)

# rename to match training names
names(prediction_stack) <- names(r_stack)

# apply the model to predict classes
classified <- predict(prediction_stack, rf_model, na.rm = TRUE)

# Define custom colors
colors <- c(
  "#a6d96a",  # hm  
  "#1a9641",  # lm  
  "#8c510a",  # md  
  "#3288bd",  # ow  
  "#fdae61",  # ph  
  "#969696",  # rd  
  "#762a83"   # up  
)



# Plot with colors
plot(classified, col = colors)

# save the output tif
writeRaster(classified, "~/Desktop/marshbirdsoutput/round_6/classified_output.tif", overwrite = TRUE)

rf_model$importance
```

# feature class importance and visualizations

```{r}
ggplot(training_data, aes(x = naip4, y = ndvi, color = class)) +
  geom_point(alpha = 0.3) +
  theme_minimal()

ggplot(training_data, aes(x = naip4, fill = class)) +
  geom_density(alpha = 0.4, color = NA) +
  theme_minimal()

imp <- as.data.frame(rf_model$importance)
imp$feature <- rownames(imp)

ggplot(imp, aes(x = reorder(feature, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Random Forest Variable Importance",
       x = "Feature", y = "Mean Decrease Gini") +
  theme_minimal()
```

# 3. Fourth Iteration: NAIP only

```{r}
# stack em up
r_stack <- c(naip)

# extract raster values for training polygons
extracted <- terra::extract(r_stack, training_polygons, df = TRUE)

# turn class into a factor
training_polygons$class <- as.factor(training_polygons$class)

# Convert training_polygons to dataframe to get the class labels
poly_df <- as.data.frame(training_polygons)
poly_df$ID <- 1:nrow(poly_df)  # Add ID column to match extract output

# Join the class labels
extracted <- extracted %>%
  left_join(poly_df[, c("ID", "class")], by = "ID")

# clean data by removing rows with na values and remove ID column
extracted_clean <- na.omit(extracted[, -1])  # remove ID and NAs

# sample 2000 per class
training_data <- extracted_clean %>%
  group_by(class) %>%
  sample_n(min(2000, n())) %>%  # Use min() to handle small classes
  ungroup()

# turn class column into factor
training_data$class <- factor(training_data$class)

```

```{r}
# split training and test data
set.seed(342)
idx <- sample(seq_len(nrow(training_data)), size = 0.8 * nrow(training_data))
train_set <- training_data[idx, ]
test_set  <- training_data[-idx, ]

# train random forest model
rf_model <- randomForest(class ~ ., 
                         data = train_set, 
                         ntree = 500)

# validation metrics
preds <- predict(rf_model, newdata = test_set)
accuracy <- mean(preds == test_set$class)
cat("Accuracy:", round(accuracy, 4), "\n")

print(rf_model)

# confusion matrix
confusionMatrix(preds, test_set$class)
```

# classifying the plots

```{r}
# load the new, unlabeled raster
prediction_raster <- rast("~/Desktop/marshbirdsoutput/round_6/prediction_raster_round_6.tif")

# calculate NDVI for prediction raster
ndvi_pred <- (prediction_raster[[4]] - prediction_raster[[1]]) / 
  (prediction_raster[[4]] + prediction_raster[[1]])
names(ndvi_pred) <- "ndvi"

# calculation NDWI
ndwi_pred <- (prediction_raster[[4]] - prediction_raster[[2]]) / 
  (prediction_raster[[4]] + prediction_raster[[2]])
names(ndwi_pred) <- "ndwi"

# Calculate brightness
brightness_pred <- (prediction_raster[[1]] + prediction_raster[[2]] + 
                      prediction_raster[[3]] + prediction_raster[[4]]) / 4
names(brightness_pred) <- "brightness"

# Apply the pca
all_for_pca_pred <- c(prediction_raster, ndvi_pred)
names(all_for_pca_pred) <- names(all_for_pca)  # ensure exact match
pca_pred <- predict(all_for_pca_pred, pca_pixels, index = 1:5)
names(pca_pred) <- paste0("PCA", 1:5)

# stack it
prediction_stack <- c(prediction_raster)

# rename to match training names
names(prediction_stack) <- names(r_stack)

# apply the model to predict classes
classified <- predict(prediction_stack, rf_model, na.rm = TRUE)

# Define custom colors
colors <- c(
  "#a6d96a",  # hm  
  "#1a9641",  # lm  
  "#8c510a",  # md  
  "#3288bd",  # ow  
  "#fdae61",  # ph  
  "#969696",  # rd  
  "#762a83"   # up  
)



# Plot with colors
plot(classified, col = colors)

# save the output tif
writeRaster(classified, "~/Desktop/marshbirdsoutput/round_6/classified_output.tif", overwrite = TRUE)

rf_model$importance
```

# feature class importance and visualizations

```{r}
ggplot(training_data, aes(x = naip4, y = naip4, color = class)) +
  geom_point(alpha = 0.3) +
  theme_minimal()

ggplot(training_data, aes(x = naip4, fill = class)) +
  geom_density(alpha = 0.4, color = NA) +
  theme_minimal()

imp <- as.data.frame(rf_model$importance)
imp$feature <- rownames(imp)

ggplot(imp, aes(x = reorder(feature, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Random Forest Variable Importance",
       x = "Feature", y = "Mean Decrease Gini") +
  theme_minimal()
```

# 5. Fifth Iteration: PCA only

```{r}
# stack em up
r_stack <- c(pca)

# extract raster values for training polygons
extracted <- terra::extract(r_stack, training_polygons, df = TRUE)

# turn class into a factor
training_polygons$class <- as.factor(training_polygons$class)

# Convert training_polygons to dataframe to get the class labels
poly_df <- as.data.frame(training_polygons)
poly_df$ID <- 1:nrow(poly_df)  # Add ID column to match extract output

# Join the class labels
extracted <- extracted %>%
  left_join(poly_df[, c("ID", "class")], by = "ID")

# clean data by removing rows with na values and remove ID column
extracted_clean <- na.omit(extracted[, -1])  # remove ID and NAs

# sample 2000 per class
training_data <- extracted_clean %>%
  group_by(class) %>%
  sample_n(min(2000, n())) %>%  # Use min() to handle small classes
  ungroup()

# turn class column into factor
training_data$class <- factor(training_data$class)

```


```{r}
# split training and test data
set.seed(342)
idx <- sample(seq_len(nrow(training_data)), size = 0.8 * nrow(training_data))
train_set <- training_data[idx, ]
test_set  <- training_data[-idx, ]

# train random forest model
rf_model <- randomForest(class ~ ., 
                         data = train_set, 
                         ntree = 500)

# validation metrics
preds <- predict(rf_model, newdata = test_set)
accuracy <- mean(preds == test_set$class)
cat("Accuracy:", round(accuracy, 4), "\n")

print(rf_model)

# confusion matrix
confusionMatrix(preds, test_set$class)
```

# classifying the plots

```{r}
# directory with your 19 subset rasters
tile_dir <- "~/Desktop/marshbirdsoutput/subset_rasters/"
tile_files <- list.files(tile_dir, pattern = "\\.tif$", full.names = TRUE)

out_dir <- "~/Desktop/marshbirdsoutput/classified_rasters/"
dir.create(out_dir, showWarnings = FALSE)

# loop through each subset raster
for (tile in tile_files) {
  message("Processing: ", tile)
  
  # load subset tile
  prediction_raster <- rast(tile)
  names(prediction_raster) <- paste0("naip", 1:4)
  
  # compute ONLY what you used in training
  ndvi_tile <- (prediction_raster[[4]] - prediction_raster[[1]]) / 
               (prediction_raster[[4]] + prediction_raster[[1]])
  names(ndvi_tile) <- "ndvi"
  
  # Match training exactly - only naip + ndvi
  all_for_pca_pred <- c(prediction_raster, ndvi_tile)
  names(all_for_pca_pred) <- names(all_for_pca)
  
  # PCA transform
  pca_pred <- terra::predict(all_for_pca_pred, pca_pixels, index = 1:5)
  names(pca_pred) <- paste0("PCA", 1:5)
  
  # match training stack
  prediction_stack <- pca_pred
  names(prediction_stack) <- names(r_stack)
  
  # output filename
  outfile <- file.path(
    out_dir,
    paste0(tools::file_path_sans_ext(basename(tile)), "_classified.tif")
  )
  
  # classify and save
  classified <- terra::predict(
    prediction_stack, rf_model,
    na.rm = TRUE,
    filename = outfile,
    overwrite = TRUE
  )
  
  plot(classified, col = colors, main = basename(tile))
}


# list all classified tif files
classified_files <- list.files(classified_dir, pattern = "\\.tif$", full.names = TRUE)

# load all classified rasters
classified_rasters <- lapply(classified_files, rast)

# mosaic them together
# do.call with terra::mosaic combines all rasters
mosaic_raster <- do.call(mosaic, c(classified_rasters, fun = "max"))

# save the mosaic
mosaic_outfile <- "~/Desktop/marshbirdsoutput/mosaic_classified.tif"
writeRaster(mosaic_raster, mosaic_outfile, overwrite = TRUE)


```

```{r} 
# Define custom colors
colors <- c(
  "#a6d96a",  # hm  
  "#1a9641",  # lm  
  "#8c510a",  # md  
  "#3288bd",  # ow  
  "#fdae61",  # ph  
  "#969696",  # rd  
  "#762a83"   # up  
)



# Plot with colors
plot(classified, col = colors)

# save the output tif
writeRaster(classified, "~/Desktop/marshbirdsoutput/round_6/classified_output.tif", overwrite = TRUE)

rf_model$importance
```

# feature class importance and visualizations

```{r}
ggplot(training_data, aes(x = PCA3, y = PCA1, color = class)) +
  geom_point(alpha = 0.3) +
  theme_minimal()

ggplot(training_data, aes(x = PCA1, fill = class)) +
  geom_density(alpha = 0.4, color = NA) +
  theme_minimal()

imp <- as.data.frame(rf_model$importance)
imp$feature <- rownames(imp)

ggplot(imp, aes(x = reorder(feature, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Random Forest Variable Importance",
       x = "Feature", y = "Mean Decrease Gini") +
  theme_minimal()
```
